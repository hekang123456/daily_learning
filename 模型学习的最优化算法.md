---
title:模型学习的最优化算法
categories: ML
tags: [梯度下降法，牛顿法，拟牛顿法]
date: 2018-12-28
---

## 模型学习的最优化算法

   ### （一）牛顿法

- 迭代公式的导出
  - 将 $f(x)$ 在 $x^{(k)}$ 处进行二阶的泰勒展开得：
    $$
    f(x) = f(x^{(k)})  + g_k (x-x^{(k)}) + \frac{1}{2} (x-x^{(k)})^T H(x^{(k)})(x-x^{(k)})
    $$
    其中 $g_k = g (x^{(k)})= \bigtriangledown f(x^{(k)})$ 是 $f(x)$ 的梯度向量在点 $x^{(k)}$ 的值。 $H(x^{(k)})$ 在 $f(x)$ 的黑塞矩阵。
    $$
    H(x) = \left[ \frac{\partial^2 f}{\partial x_i \partial x_j} \right]_{n \times n}
    $$
    将泰勒展开进行变换得：
    $$
    \frac{f(x) - f(x^{(k)})}{x-x^{(k)}} = g_k + \frac{1}{2} H(x^{(k)})(x-x^{(k)})
    $$
    当 $（x-x^{(k)}） \rightarrow0$ 时：<font color=red size=6> $\bigtriangledown f(x) = g_k + H_k (x-x^{(k)})$</font>这里忽略了 $\frac{1}{2}$ 可能是只最求符号的一致吧！

  - 令 $\bigtriangledown f(x^{(k+1)})=0$ ：

    因为某个点为极值点的必要条件是在极值点的一阶导数为零，所以这样做是希望 $x^{(k)}$ 能够逼近导数值为零的点。另$\bigtriangledown f(x^{(k+1)})=0$  可以得到：

    $$
    x^{(k+1)} = x^{(k)} - H_k^{-1} g_k
    $$
- 牛顿法的流程

  - 初值 $x^{(0)}, k=0$
  - 计算 $g_k$ , 若 $||g_k||<\epsilon$ 则停止计算。
  - 计算 $H_k$ 和 $H_k^{-1}$。
  - 计算 $x^{(k+1)}$, $k=k+1$ 转第二步。



### (二) 拟牛顿法

​	**目标：** 寻求一个$n$ 阶的矩阵 $G_k=G_k (x^{(k)})$ 来近似的代替黑塞矩阵的逆 $H^{-1} = H^{-1} (x^{(k)})$ 。

​       **拟牛顿条件：** 根据(一)中加大的公式可知， $g_{k+1} -g_k = H_k (x^{(k+1)} - x^{(k)}) $ 。这就是拟牛顿条件， 可以理解为导数的一阶差分等于黑塞矩阵乘以自变量的差。

​	**拟牛顿法中的矩阵需要满足的条件：**

​		1).  $G_k $ 和 $H_k^{-1}$ 一样需要满足正定的条件， 因为只有 $G_k$ 是正定的时候，能够保证 $-H_k^{-1}g_k$ 的方向是朝着 $f(x)$ 的值是降低的。

​		2). 满足拟牛顿条件， 

#### 2.1 DFP (Davidon-Fletcher-Powell) 算法 

#### 2.2 BFGS 算法

#### 2.3 Broyden 类算法



### (三) 梯度下降法

- 迭代公式的导出：	

  - 一阶泰勒展开
    $$
    f(x) = f(x^{(k)}) + g_k^T (x-x^{(k)})
    $$
    其中 $g_k = g (x^{(k)})= \bigtriangledown f(x^{(k)})$ 是 $f(x)$ 的梯度向量在点 $x^{(k)}$ 的值。

  - 步长 $\lambda_k$的选择
    $$
    f(x^{(k)} + \lambda_k p_k) = \min\limits_{\lambda \geq 0} f(x^{(k)} + \lambda p_k )
    $$
    其中 $p_k$ 是搜索方向为 $p_k = - \bigtriangledown f(x^{(k)})$ , 通过一维线性搜索确定  $\lambda_k$ 的值。

- 梯度下降算法

  - 取初始值 $x^{(0)} \in R^n, k=0$;

  - 计算$f(x^{(k)})$;

  - 计算 梯度 $g_k = g(x^{(k)})$ ,当 $||g_k||< \epsilon$ , 停止迭代；否则另 $p_k = -g_k$ ， 求 $\lambda_k$ , 使：
    $$
    f(x^{(k)}+\lambda_k p_k) = \min\limits_{\lambda \geq 0} f(x^{(k)} + \lambda p_k)
    $$

  - $x^{(k+1)} = x^{(k)} + \lambda_k p_k $, 计算 $f(x^{(k+1)})$ 

    当 $||f(x^{(k+1)}) - f(x^{(k)})||<\epsilon$ 或者 $||x^{(k+1)}-x^{(k)}||<\epsilon$ 时， 停止迭代。

  - 令 $k=k+1$, 转第三步。

