---
title:SVM简要整理
categories: ML
tags: [SVM，SMO算法]
date: 2018-12-25
---

### SVM简要整理

#### 线性支持向量机

- SVM 的分类（3种）

- SVM 的导出 (函数软间隔，几何间隔， 间隔最大化)

- SVM 的硬间隔最大化的规划问题
  $$
  \begin{align}
  & \min\limits_{w,b} \frac{1}{2}||w||^2 \\
  &s.t. \quad y_i \left( w \cdot x_i  + b \right) \geq 1, i=1,2,...,N
  \end{align}
  $$
  
- SVM 软间隔最大化的规划问题
  $$
  \begin{align}
  \min\limits_{w,b,\xi}\quad &  \frac{1}{2}||w||^2 + C\sum\limits_{i=1}^N \xi_i \\
  s.t. \quad & y_i \left( w \cdot x_i  + b \right) \geq 1-\xi_i, i=1,2,...,N \\
  &\xi_i \geq 0, i=1,2,...,N
  \end{align}
  $$

-  SVM 求解过程

  - 目标式-->拉格朗日函数-->极小极大-->极大极小

  - 对$w,b, \xi$求导令其为零得到
    
    | 线性SVM                                                      | 软间隔SVM                                                    |
    | ------------------------------------------------------------ | ------------------------------------------------------------ |
    | $\begin{align}<br/>\sum\limits_{i=1}^N \alpha_i y_i x_i &=w\\<br/>\sum\limits_{i=1}^N \alpha_i y_i &= 0\\<br/>\end{align}$ | $\begin{align}<br/>\sum\limits_{i=1}^N \alpha_i y_i x_i &=w\\<br/>\sum\limits_{i=1}^N \alpha_i y_i &= 0\\<br/>C-\alpha_i -u_i &=0<br/>\end{align}$ |
    
  - 将上一步得到的条件带入得到不含原目标式参数的求极大化的对偶问题

  - 改为级小化，得到最终的对偶最优化问题。
    
    | 硬间隔SVM                                                    | 软间隔SVM                                                    |
    | ------------------------------------------------------------ | ------------------------------------------------------------ |
    | $\begin{align}<br/>\min\limits_\alpha  \quad& \frac{1}{2} \sum\limits_{i=1}^N \sum\limits_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) - \sum\limits_{i=1}^N \alpha_i \\<br/>s.t. \quad& \sum\limits_{i=1}^N \alpha_i y_i = 0 \\<br/>& \alpha_i \geq 0, i=1,2,...,N<br/>\end{align}$ | $\begin{align}<br/>\min\limits_\alpha  \quad& \frac{1}{2} \sum\limits_{i=1}^N \sum\limits_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) - \sum\limits_{i=1}^N \alpha_i \\<br/>s.t. \quad& \sum\limits_{i=1}^N \alpha_i y_i = 0 \\<br/>& 0\leq \alpha_i \leq C, i=1,2,...,N \\<br/>\end{align}$ |

- **支持向量:** 对应$\alpha_i^* > 0$ 的样本点称为支持向量。

- 合页损失函数。



#### 非线性支持向量机与核函数

- 常用的核函数
  - 多项式
  - 高斯
  - 字符串
- 非线性支持向量机学习算法
  - 直接构造带有核函数的最优化问题
  - 求解（SMO）
- 序列最小最优化算法(SMO 算法)
  - 初值
  - 优化变量的选择
  - 停机条件的判断
  - 确定最优化解
- 两个变量二次规划的求解方法
  - 计算上下界
  - 不考虑上下界的最优解
  - 考虑上下界的最优解
- 变量的选择
  - 第一个变量的选择（偏离KKT条件最大）
  - 第二个变量的选择（$|E_1 - E_2|$最大的那个） 
  - 计算 $b$ 和 $E_i$



### 支持向量机的个人理解

支持向量机是定义在特征空间上的间隔最大化的线性分类器。 目标是使训练数据上 最小的几何间隔 最大化。考虑到几何间隔和函数间隔之间的关系，可以发现最小的函数间隔的取值对构造的优化问题是没有影响的。 因此可以定义最小的函数间隔为1。 并且因为最大化二范数的倒数等价于最小化二范数，因此可以得到一个标准的凸二次规划的问题。  可以通过拉格朗日对偶性，构造原问题的对偶问题来求解。 求解的流程是（1）构造拉格朗日函数，（2）将极小极大问题改为极大极小化问题. （3）先对 $w,b$ 求导为0, 得到两组等式。（4）将等式带入拉格朗日函数得到对偶问题。  通常对偶问题比原问题更容易求解（不等式约束变成了等式约束），而且很自然的引入了核函数。 通过对偶问题我们可以发现对支持向量机的分离决策面其影响的点只有那些在拉格朗日函数中对应的权重值大于零的点，这些点也是使得原问题的约束条件等号成立的点， 这些点称为支持向量。 因此改变除支持向量以外的点对分离超平面是没有影响的。 



对于非线性可分的数据，可以采用软间隔以及核函数的方法来解决。 



软间隔的方法是在硬间隔SVM 的基础上对每个样本点引入一个松弛变量，使得函数间隔加上松弛变量大于等于1。对于目标函数，因为每个松弛变量的引入都需要支付一个代价，因此选哟增加一个惩罚项。 其中有一个惩罚参数 $C$ 需要人为的去设定。 引入了软间隔的SVM的求解和硬间隔SVM的求解过程是一样的。



对于线SVM来说，其等价于合页损失函数最小。



对于非线性可分的数据也可以通过将原数据通过函数变换映射到高维线性可分空间再利用线性SVM进行求解。 通过对对偶问题的分析可以知道我们不必显示的定义映射，只需要知道映射的内积就行了， 因为核函数就是映射函数的内积。 也就是说我们可以通过定义核函数借助对偶问题直接求解经过非线性变换之后的SVM。 引入了核函数的SVM的求解与线性SVM的求解唯一的不同在于首先需要确定一个核函数， 常用的核函数包括了 多项式核函数，高斯核函数以及字符串核函数。 



对偶问题的求解（凸二次规划问题的求解），我们可以采用序列最小最优化算法进行求解（SMO）。SMO算法是一种启发式的算法，需要我们首先固定所有其他的样本点不变，选取两个变量进行优化（构造了只有两个变量的二次规划问题）。该优化过程的停止条件是判断所有的变量是否都满足KKT条件。 KKT条件是 判断对偶问题的解是否为原问题的解的一个充要条件。所以这个算法有两个主要的问题，一个是只有两个变量的二次规划问题的求解另一个是两个变量的选择方法。