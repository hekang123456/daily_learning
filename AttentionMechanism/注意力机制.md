---
title: Attention Mechanism
categories: Deep Learning
tags: [Attention Mechanism, 注意力机制]
date: 2019-03-14
---

### 注意力机制

####  （一）. 一般的注意力机制的公式

$$
\begin{align}
S^{l \times 1} &= \tanh (H^{l \times m} W^{m \times n} +b^{n})U^{n \times 1} \\
\alpha^{l \times 1} &= \frac{ \exp (S^{l \times 1}) }{ \sum\limits_{i=1}^{l} \exp （S_{i,1}^{l \times 1}）} \\
v &= \sum\limits_{i=1}^l \alpha_{i, 1} H^{l \times m}_{i，：} 
\end{align}
$$

> 在上面的公式中 上标表示的是符号对应的矩阵的形状， 下标指的是取这个矩阵中的具体某一个元素， 冒号表示选取那个维度的所有元素。 

- 第一个公式表示的是打分函数的计算， 原始的输入 $H^{L \times m}$ 表示为 $l \times m $ 的一个矩阵。 例如有 $l$ 个词的文本，每个文本的词嵌入是 $m$ 维； 每个时间步 RNN 的输出的合并，$l$ 表示时间步的长度， $m$ 表示每个时间步输入向量的维度。  首先将 $H$ 经过一个 全连接层 (dense layer ， 就是两个矩阵相乘)  将 $m$ 维的向量变换为 $n$ 维的向量。再经过一个激活函数， 然后再经过一个全连接层。 得到$S^{l \times 1}$ ，称为是对每个输入特征向量的打分。 类似的打分函数还有：
  $$
  \begin{align}
  S^{l \times 1} &= H^{l \times m}U^{m \times 1} \\
  S^{l \times 1} &= H^{l \times m}W^{m \times n} U^{n \times 1} \\
  S^{l \times l} &= \tanh (H^{l \times m }) U^{m \times 1}  
  \end{align}
  $$
  简单一点理解就是将 输入 $H^{l \times m}$ 通过矩阵运算得到一个 $S^{l \times 1}$ 的矩阵， 这个矩阵中的每个值代表了对应位置特征向量的重要程度。  公式中的$W, b, U$ 都是权重，在注意力网络中通过随机初始化，然后通过反向传播进行训练的。

- 第二个公式表示的是一个 Softmax 函数。 将得到的 $S^{l \times 1}$ 看做一个向量， 那么这就是对这个向量的归一化的过程。

- 第三个公式表示根据第二步确定的每个特征向量的重要程度，对原始输入 $H$ 中的特征向量进行加权求和。 

- 举个例子：

  -  输入 $H = [[1,2,3], [2,2,2], [1,1,1], [2,1,2]]$  ，$W, b, U$ 分别初始化为 $W = [[1,1], [2,2], [1,2]]， b=[[1, 1]], U=[[2], [1]]$

  - code

    ```python
    import numpy as np
    H = np.array([[1,2,3], [2,2,2], [1,1,1], [2,1,2]])
    W = np.array([[1,1], [2,2], [1,2]])
    b = np.array([[1, 1]])
    U = np.array([[2], [1]])
    
    S = np.dot(np.dot(H, W)+b, U)
    alpha = np.exp(S)/sum(np.exp(S))
    res = np.dot(np.array([[1,1,1,1]]), H* alpha)
    ```

#### (二). seq2seq 中的 Attention

