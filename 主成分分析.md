---
title: 主成分分析方法
categories: ML
tags: [PCA， 主成分分析方法，降维]
date: 2019-01-05
---

### 主成分分析（Principal Component Analysis ）

#### 什么是主成分

​	对高维的数据点进行变换到另一组低纬的空间中， 并且使得数据点在低纬的空间中也能够表达出原始数据的意义。 就称为低纬空间中的对应变换坐标所包含的信息为主成分。 例如三维映射到二维的 x轴和y轴。那么x轴和y轴所包含的信息为我们找到的主成分。



#### 主成分分析的步骤

- 对数据点进行中心化处理(为了让投影后的均值也为0，方便后面的计算)

- 写出变换后的方差。（投影后方差最大）
  $$
  \begin{align}
  D(x) &= \frac{1}{n} \sum\limits_{i=1}^n (x_i^T w-0)(x_i^Tw-0) \\
  &= \frac{1}{n} \sum\limits_{i=1}^n w^T x_i x_i^T w \\
  &= w^T (\frac{1}{n} \sum\limits_{i=1}^n x_i x_i^T) w
  \end{align}
  $$

- 目标函数为：
  $$
  \begin{align}
  \max &\quad w^T \Sigma w \\
  s.t &\quad w^Tw = 1
  \end{align}
  $$
  

- 引入拉格朗日函数函数
  $$
  L(w, \lambda) = w^T \Sigma w + \lambda (1-w^Tw)
  $$

- 求 $\frac{ \partial L }{ \partial w}$ 与 $\frac{\partial L}{ \partial \lambda}$ 
  $$
  \Sigma w = \lambda w
  $$
  即求对应的特征值

- 对协方差矩阵进行特征值分解，将特征值按从大到小的顺序排列

- 取特征值前 $d$ 大对应的特征向量 $w_1, w_2, ..., w_d$ 通过以下映射将 $n$ 维样本映射到 $d$ 维
  $$
  x_l' = 
  \left[
  \begin{align}
  w_1^T x_l \\
  w_2^T x_l \\
  ..... \\
  w_d^T x_l
  \end{align}
  \right]
  $$
  信息占比为： $\eta = \sqrt{ \frac{\sum\limits_{i=1}^d \lambda_i^2}{ \sum\limits_{i=1}^n \lambda_i^n } }$

#### 主成分分析方法的两种解释

-  最大方差的解释
- 最小平方误差（最小化样本点到直线的距离平方和, 百面机器学习第一版p79）