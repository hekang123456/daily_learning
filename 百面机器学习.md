## 第四章: 降维

### 4.1 PCA 最大方差理论

- 如何定义主成分？
  利用空间变换， 利用较低维度数据最大程度的表征原始数据，从而达到降维的目的。

- 如何设计目标函数进行主成分的提取？
  给定一堆数据点  $(x_1, x_2, ..., x_n)$  我们将其通过线性变换可得  $w^Tx_ i $ （此处向量均为列向量） 这个变换可以理解为是 $x_i$ 投影在 $w$ 向量方向得到的向量。 因此投影得到的向量的长度(方差)是 $w^Tx_ix_i^Tw$ , 对所有的 $x_i$ 进行变换后求和可得：
  $$\sum_i w^Tx_ix_i^Tw=  w^T (\sum_i x_i x_i^T) w$$   
  在 $w​$ 只表征方向的情况下，我们希望这个值越大越好。因此可以得到以下优化问题：
  $$\left\{ \begin{align}  \max & w^T (\sum_i x_i x_i^T) w \\ s.t.  & w^Tw=1 \end{align}\right.$$

- 针对这个目标函数如何对PCA 问题进行求解
  - 对样本数据进行**中心化**处理
  - 求样本的**协方差矩阵**
  - 协方差矩阵进行**特征值分解**， 将特征值从大小到小排列
  - 取特征值**前 $d$ 大**对应的特征向量 $w_1, w_2,..., w_d$.
  - 将 $n$ 维的样本映射到$d$ 维。

- 降维后的信息占比为
$$\eta = \sqrt{\frac{\sum_{i=1}^d \lambda_i^2}{\sum_{i=1}^n \lambda_i^2} }$$

### 4.2 PCA 最小平方误差理论
- 从回归的角度来定义 PCA 并求解
  - 定义每个点 $x_k$ 到超平面 $D$ 的距离为： 
    $$\mbox{distance}(x_k, D) = \parallel x_k-\tilde{x_k} \parallel_2$$
    其中 $\tilde{x_k}$ 表示 $x_k$ 在超平面上的投影向量，并且假设 $D$ 是由 $d$ 个标准正交基 $W=\{w_1, w_2, ...,w_d\}$ 构成。 
  - 投影向量 $\tilde{x_k}$ 用这组标准正交基线性表示：
    $$\tilde{x_k}=\sum\limits_{i=1}^d (w_i^Tx_k)w_i$$
  - PCA 的优化目标函数
    $$\left\{ \begin{align} \arg\min\limits_{w_1, w_2, ... w_d}  & \sum\limits_{k=1}^n \parallel x_k - \tilde{x_k}  \parallel_2^2   \\ s.t & w_i^T w_j = \delta_{ij} =\left\{ 1, i=j  ;0, i\neq j  \right. \end{align}\right.$$
  - 将  $$\tilde{x_k}=\sum\limits_{i=1}^d (w_i^Tx_k)w_i$$ 带入上述公式得到：
$$
\left\{
	\begin{align}
		\arg\max_W & tr(W^T XX^T W),\\
		s.t. & W^TW= I.
	\end{align} 
\right.
$$
 		该公式和4.1中的目标相似。计算方法一样。

### 4.3 线性判别分析
PCA 是一种无监督的降维方法，LDA 是一种有监督的降维的方法，其将标签信息考虑进去了。
-  如何设计目标函数使得降维的过程中不损失类别信息？
	- 可以采用考虑线性判别分析的方法： 最大化类间距离和最小化类内距离
	- 目标函数是：
$$J(w) = \frac{w^T(u_1-u_2)(u_1-u_2)^Tw}{\sum_{x \in c_i} w^T (x-u_i)(x-u_i)^Tw}$$
-  在这种目标下，如何进行求解？
	- 定义类间散度矩阵 $S_B = (u_1-u_2)(u_1-u_2)^T$ , 类内散度矩阵是 $S_w=\sum_{x \in c_i }(x-u_i) (x-u_i)^T$ 。 则目标函数可以写成：
$$J(w) = \frac{w^T S_B w}{w^T S_w w}$$  
	- 对 $w$ 进行求偏导然后令 $\lambda = J(w)$ 可以得到：
$$ S_w^{-1} S_Bw = \lambda w $$
	所以目标就成了求解 $S_w^{-1}S_B$ 最大的特征值问题， 而投影方向就是这个特征值对应的特征向量。
- 多类别的线性判别分析怎么做？
类间距离$S_b = \sum\limits_{j=1}^N m_j (u_j-u)(u_j-u)^T$, 表示每个类的中心和所有样本的中心的距离。 $S_w$ 不变。$m_j$ 表示第 $j$ 个类别中的样本个数。
因此将最大化的目标是：
$$J(W) = \frac{tr(W^TS_bW)}{tr(W^TS_wW)} $$
最后的求解目标变成了广义特征值的求解
$$S_b w = \lambda S_w w$$

### 4.4 线性判别分析和主成分分析
- 如何从应用的角度分析 LDA 和 PCA 的异同
	- 目标： PCA 选择投影后数据方法最大的方向（无监督）， LDA 利用了标签的信息选择 类内方差最小而类间方差最大。
	- 应用
		- PCA:  语音去噪
		- LDA:  从语音中选择是哪个人的声音

## 第五章：非监督学习
### 5.1 K 均值聚类
- 简述 K 均值聚类的具体步骤（**hk: 一种EM算法**）？
  - 数据**预处理**，数据归一化， 离群点的处理
  - 随机选取 $K$ 个族**中心**： $u_1^{(0)}, u_2^{(0)}, ..., u_K^{(0)}$
  - 定义**代价函数**为： $J(c,u) = \min\limits_u \lim\limits_c \sum\limits_{i=1}^M \parallel x_i - x_{c_l} \parallel^2$
  - **重复**下面的过程直到 $J$ 收敛。

    - 对于每个样本 $x_i$ 将其分配到距离最近的族。
       $$
       c_i^{(t)} \leftarrow \arg \min_k \parallel x_i - u_k^{(t)} \parallel^2;
       $$

    - 对于每个类族 $k$, 重新计算该类的族的中心
        $$
        u_k^{(t+1)} \leftarrow \arg\min_u \sum \parallel x_i -u \parallel^2
        $$

- $K$ 均值算法的优缺点是什么？
  - 缺点
    - 结果受初值和和离群点的影响不稳定。
    - 结果通常不是全局最优而是局部最优
    - 无法很好的分类拒族分布差别比较大的情况（例如：一个类的样本是 另一个的100倍）
  - 优点
  	- 时间复杂度为 $O(NKt)$ $N$ 是数据对象的数目， $K$ 是族数， $t$ 是迭代次数。接近于线性。
- $K$ 均值算法的调优算法？
	- 数据归一化和离群点处理
	均值和方差大的维度将对数据的聚类结果产生决定性的结果
	- 合理选择$K$值
		- 手肘法 （横轴为$K$值，纵坐标为损失）
		- Gap Statistic 方法。（蒙特卡洛模拟）
		- 采用核函数
- 针对 $K$ 均值算法的缺点，有哪些改进的模型？
	- $K$ 均值算法的主要缺点如下：
		- 需要人工预先确定**初始** $K$ 值， 且该值和真实的数据分布未必吻合。
		- $K$ 均值只能收敛到局部最优， 效果受到**初始值**影响很大。
		- 易受噪点的影响。
		- 样本只能被划分到单一的类中。
	- 改进初始值对结果的影响
		- K-means++ 算法
		选取的第 $K$ 个值要尽量的远离前 $K-1$ 个已经选择好了的点。
	- 改进的 $K$ 值选择方法
		- ISODATA 算法 （迭代自组织数据分析方法）
		当属于某个类别的数据点过少的时候就把该类别去除（**分离操作**）， 当属于某个类别的样本数过多、分散程度较大的时候，把该类别分成两个子类别（**合并操作**）。 该方法所需的超参数：1. 预期的聚类中心数目 $K_0$; 2. 每类所要求的最少样本数目 $N_{\text{min}}$; 3. 最大方差 Sigma; 4. 两个聚类中心所允许的最小距离 $D_{\text{min}}$。
		
### 5.2 高斯混合模型
理论上高斯混合分布可以拟合出任意类型的分布。
- 高斯混合模型的核心思想是什么？
假设数据可以看做是从多个高斯混合分布中生成出来的。高斯混合模型（hk:应该是做了独立同分布的假设的。）的公式为：
$$
p(x) = \sum\limits_{i=1}^K \pi_i N(x|u_i, \sigma_i)
$$

- 高斯混合模型是如何迭代计算的？
	- EM 算法的迭代过程
		- E步骤： 根据当前的参数，计算每个点由某个分模型生成的概率。
		- M步骤： 使用E步估计出的概率，来改进每个分模型的均值，方差和权重。 
- 高斯混合模型与 $K$ 均值算法的异同?
	- 共同点： 1. 都可用于聚类算法； 2.都采用EM算法来求解；3. 都往往只能够收敛到局部最优；
	- 不同点： 1. 高斯混合分布能够给出一个样本属于某个类的概率； 2. 高斯混合分布还可以用于概率密度的估计；3. 高斯混合分布能够用于生成新的样本点。

### 5.3 自组织映射神经网络（self-origanizing map, SOM）
- 自组织映射神经网络是如何工作的？
  SOM 本质上是一个两层的神经网络，包括输入层和输出层（竞争层）；这是一种无监督的学习算法。
  - 初始化： 所有的连接权重都用小的随机值进行初始化；
  - 竞争： 计算具有最小判别函数值的特定神经元为胜利者； 每个神经元 $j$ 的判别函数为 $d_j(x) = \sum\limits_{i=1}^D (x_i-w_{i,j})^2$。
  - 合作： 获胜的神经元 I(x) 决定了兴奋神经元拓扑邻域的空间位置，计算邻近神经元的更新程度 : $T_{j, I(x)}(t) = \exp\left( -\frac{S^2_{j, I(x)}}{2\sigma(t)^2} \right)$, $S_{i,j}$表示竞争层神经元$i$ 和 $j$ 之间的距离。 $\sigma(t) = \sigma_0 \exp(-\frac{t}{\tau_\alpha})$
  - 适应： 调整兴奋神经元的连接权重： $\Delta w_{ji} = \eta (t) \cdot T_{j,I(x)}(t)\cdot(x_i - w_{ji})$, 调整学习率： $\eta(t) = \eta_0 \exp\left( -\frac{t}{\tau_\eta} \right)$
  - 迭代： 知道特征映射趋于稳定
- 自组织映射神经网络与K均值算法有何区别？
  - K 均值算法需要事先定下类的个数，而 SOM 中聚类的结果可能小于神经元的个数。
  - K 均值算法为每个输入数据找到相似的类之后，只更新这个类的参数，而SOM会更新附近点的参数。
  - SOM 的可视化比较好，而且具有优雅的拓扑关系图。

- 怎样设计自组织映射神经网络并设定网络训练参数？
  - 设定输出层神经元的数量
    该值和样本类别相关， 在不清楚的情况下可以设定较多的节点数。以便较好的映射样本的拓扑结构。
  - 设计输出层节点的排序
    这东西需要考虑实际问题的物理意义； 例如一般的分类问题一个输出节点可以代表一个模式类，颜色空间和旅行路径问题，二维平面空间比较直观。
  - 权值的初始化
    尽量使权值的初始化位置与输入样本的大概分布区域充分重合。比较简单的方法有从训练数据中随机选取 $m$ 个样本作为初始权重； 也可以随机初始化。
  - 设计拓扑领域
    领域不断缩小， 各种拓扑形状都可以（正方形，圆形，六边形...）
  - 设计学习率
    学习率是一个递减的函数

### 5.4 聚类算法的评估
- 假设没有外部标签数据，如何评估两个聚类算法的优劣
  - 常见的数据簇： 1. 中心； 2. 密度； 3. 连通； 4. 概念。
  - 聚类评估过程
    - 估计聚类的趋势： 1. 聚类误差是否随类别增加而线性增加； 2. 霍普金斯统计量。
    - 判断数据簇数： 1. 手肘法； 2. Gap Statistic 方法
    - 测量聚类质量： 1. 轮廓系数； 2. 均方标准偏差； 3. R方； 4.改进的hubert统
## 第六章: 概率图模型
### 6.1 概率图模型的联合概率分布
- 如何写出 6.1(a) 中贝叶斯网络的联合概率分布？（菱形带方向）
- 如何写出 6.1(b) 中马尔可夫网络的联合概率分布？（菱形不带方向）
最大团： 由一个节点构成的子集中，任意两点之间都存在边相连，则这个子集中的所有节点构成了一个团。

### 6.2 概率图表示
- 解释朴素贝叶斯模型的原理。
预测指定样本属于特定类别的概率 $P(y_i |x)$ 来预测该样本的所属类别， 即：
$$y=\max\limits_{y_i} P(y_i |x )$$
$P(y_i|x)$ 可以写成： $P(y_i|x) = \frac{P(x|y_i)P(y_i)}{P(x)}$

- 给出**朴素贝叶斯模型的概率图模型**？

- 解释最大熵模型的原理，并给出概率图模型的表示？
  - 分布 $P(x)$ 的熵定义是： $H(P) = -\sum\limits_x P(x)\log P(x)$
  - 条件熵： $H(P)=-\sum\limits_{x,y} \tilde{P}(x) P(y|x) \log P(y|x)$
  - 最大熵模型的学习等价于约束最优化问题：
    $$
    \begin{align}
    \max_P H(P) &= -\sum\limits_{x,y} \hat{P}(x)P(y|x)\log P(y|x), \\
    s.t., E_{\hat{p}}(f_i) &= E_p (f_i), \forall i=1,2,...,M,\\
    \sum\limits_y P(y|x) &= 1.
    \end{align}
    $$

  - 求解之后得到的最大熵模型的表示形式为
  $$
  P_w(y|x) = \frac{1}{Z} \exp(\sum\limits_{i=1}^M w_i f_i(x,y))
  $$
  - **最大熵模型的概率图**： （无向图模型）
  
### 6.3 生成式模型与判别式模型
- 常见的概率图模型中，哪些是生成式模型， 哪些是判别式模型？
  - 判别式模型：
    判别式模型是直接对条件概率分布 $P(Y,Z|X)$ 进行建模， 然后消掉无关变量 $Z$ 就可以得到对变量集合 $Y$ 的预测，即 $P(Y|X) = \sum\limits_z P(Y,Z|X)$
  - 生成式模型：
    生成式模型对联合概率分布 $P(X,Y,Z)$ 进行建模 $P(Y|X) = \frac{P(X,Y)}{P(X)} = \frac{\sum_z P(X,Y,Z)}{\sum_{Y,Z}P(X,Y,Z)}$
  - 常见的生成式模型： <font color=red>**朴素贝叶斯、贝叶斯网络、pLSA、LDA、隐马尔可夫模型**</font>。
  - 常见的判别式模型： 最大熵模型、条件随机场。
### 6.4 马尔可夫模型
- 如何对中文分词问题用隐马尔科夫模型进行建模和训练？
  - 马尔科夫模型的三个问题： <font color=red>**1. 概率计算问题 （前向后向算法）； 2. 预测问题（维特比算法）； 3.学习问题（Baum-Welch算法）** </font>
  - 问题： 将中文分词问题视为是一个序列标注问题。(隐状态是每个字的标注)
- 最大熵马尔可夫模型为什么会产生标注偏置问题？ 如何解决？
  - 最大熵马尔可夫模型

    - 隐马尔可夫模型是对隐状态序列和观测状态序列的联合概率 $P(x,y)$ 进行建模的生成式模型，而最大熵马尔可夫模型是直接对标注的后验概率 $P(y|x)$ 进行建模的判别式模型。

  - 最大熵模型存在的标注偏置问题（P131）

    - 问题：

      由于局部归一化的影响，隐状态会倾向于转移到那些后续状态可能更少的状态上，以提高整体的后验概率， 这就是标注偏置问题。 

    - 解决方法

      条件随机场的归一化因子是在全局范围内进行归一化，枚举了整个隐状态序列 $x_{1...n}$ 的全部可能，从而解决了局部归一化带来的标注偏置问题。

### 6.5 主题模型

将具有相同主题的词或词组映射到同一纬度上去。

- 常见的主题模型有哪些？ 试介绍其原理。

  - pLSA (Probabilistic Latent Semantic Analysis) **生成模型**

    - 符号说明： $d$: 文章； $z$: 主题； $w$:词

    - 给定文章生成词的概率是：

       $p(w|d)=\sum\limits_z p(w|z, d)p(z|d)$ 假设在给定主题 $z$ 的情况下，生成词 $w$ 的概率与特定的文章无关则可得：$p(w|d) = \sum\limits_z p(w|z)p(z|d)$ 

    - 整个语料库中文本生成概率用似然函数表示为:

      $$L = \prod\limits_m^M \prod\limits_n^N p(d_m,w_n)^{c(d_m, w_n)}$$

    - Log 似然函数可以写为：

      $$\begin{align} l&=\sum\limits_{m}^M\sum\limits_n^Nc(d_m, w_m)\log p(d_m,w_n)\\ &=\sum\limits_m^m \sum\limits_n^N c(d_m,w_n)\log \sum\limits_k^K p(d_m) p(z_k|d_m)p(w_n|z_k) \ \end{align}$$

      其中 $p(w_n|z_k)$ 和$p(z_k|d_m)$是待估的参数。 

    - 求解方法 : 最大期望算法。

  - LDA  

    LDA 采用的是贝叶斯学派的思想认为带估计的参数（主题分布和词分布）不再试一个固定的常数，而是服从一定分布的随机变量。

    语料库的生成过程： 对文本库中的每一篇文档 $d_i$ ，采用以下操作：

    - 从超参数为 $\alpha$ 的狄利克雷分布中抽样生成文档 $d_i$ 的主题分布$\theta_i$
    - 对文档 $d_i$ 中的每一个词进行以下3个操作：
      1.  从多项式分布 $\theta_i$ 中抽样生成它所对应的主题 $z_{ij}$
      2. 从超参数为 $\beta$ 为狄利克雷分布中抽样生成主题 $z_{ij}$ 对应的词分布 $\psi_{z_{ij}}$ 
      3. 从代表词的多项式分布 $\psi _{z_{ij}}$ 中抽样生成词 $w_{ij}$
    - 求解方法：
      - **吉布斯采样（Gibbs Sampling）**

- 如何确定 LDA 模型中的主题

  - 训练数据的分类： 训练集 $60\%$, 验证集$20\%$, 测试集$20 \%$ 。

  - 评价指标： (困惑度 perplexity)

    $$\text{perplexity}(D) = \exp \left\{  -\frac{\sum\limits_{d=1}^M \log p(w_d)}{\sum\limits_{d=1}^N N_d} \right\}$$

    其中 $M$ 是文档的总数， $w_d$ 为文档 $d$ 中单词所组成的词袋向量， $p(w_d) $ 为模型所预测的文档 $d$ 的生成概率，  $N_d$ 为文档 $d​$ 中单词的总数。

  - **第一种方法：** 选择当主题数增加困惑度指标在训练集上继续下降但在验证集上反而增长的主题个数作为超参数。
  - **第二种方法：** 在 LDA 的基础上融入分层狄利克雷过程，构成一种非参数主题模型 HDP-LDA。 这种方法不需要预先指定主题的个数，模型可以随着文档的数目变化而对主题个数进行调整，缺点是会使概率图模型变得更加复杂。

- 如何用主题模型解决推荐系统中的冷启动问题？

  - 目标： 优化点击率、转化率或用户体验（用户留存时间、留存率等）
  - 类别： 用户冷启动、物品冷启动、系统冷启动。

## 第八章： 采样

### 8.1 采样的作用

- 举例说明采样在机器学习中的应用
  - 给定概率分布，模拟产生随机事件
  - 用少量的样本点（经验分布）来近似总体分布
  - 重采样。（处理数据不平衡问题）
  - 对复杂模型进行随机模拟，进行近似求解或推理

- 什么是自助法和刀切法
  - 自助法： 从给定训练数据集中有放回的均匀采样。
  - 刀切法：每次从样本集中删除一个或者几个样本，剩余的样本成为“刀切”样本。

### 8.2 均匀分布随机数

- 如何编程实现均匀分布随机数生成器？

  - 线性同余法：  $x_{t+1}=(a \cdot x_t + c) \mod m$ 

  - 伪随机数，该算法最多只能产生 $m​$ 个不同的随机数。 

  - 多次产生随机数之后该方法得到的随机数序列会进入循环周期。 

  - gcc 中的 glibc 设置的 超参数值为：

    $$m=2^31-1,\\a=1103515245, \\c=12345​$$

  - 真正的随机数只存在于自然界的物理现象中。

- 线性同余法中的随机种子一般如何选定？

  - 当前的系统时间

### 8.3 常见的采样方法

- 抛开那些针对特定分布而精心设计的采样方法，说一些你所知道的通用采样方法或采样策略，简单描述它们的主要思想以及具体操作步骤。

  - 直接用均匀采样的一些扩展方法来产生样本点： 有限离散分布可以用轮盘赌算法来采样。

  - 函数变换采样法：

    - $u=\varphi(x)$ , 则他们的概率密度函数有如下关系 $p(u)|\varphi'(x)|=p(x)$ 
    - $u$ 采样简单， 能够得到复杂的分布。 

  - 逆采样法：

    - 假设待采样的目标分布的概率密度函数为 $p(x)$, 他的累积概率分布函数是：

      $$u=\Phi(x) = \int_{-\infty}^x p(t)d_t $$

      (1) 从均匀分布 $U(0, 1)$ 产生一个随机数 $u_i$;

      (2) 计算 $x_i = \Phi^{-1} (u_i) $, 其中 $\Phi^{-1}(\cdot)$ 是累积分布函数的逆函数。

  - 拒绝采样（Rejection Sampling）

    对于目标分布 $p(x)$ , 选取一个容易采样的参考分布 $q(x)$ ,使得对于任意$x$ 都有 $p(x) \leq Mq(x)$ 则可以按如下过程进行采样：

    (1) 从参考分布 $q(x)$ 中随机抽取一个样本 $x_i$ ;

    (2) 从均匀分布 $U(0, 1)$ 产生一个随机数 $u_i$;

    (3) 如果 $u_i  < \frac{p(x_i)}{ Mq(x_i)}$ ，则接受样本 $x_i$; 否则拒绝。

  - 自适应拒绝采样

    同拒绝采样的方法，只不过采用分段线性函数来覆盖目标分布的对数 $\ln p(x)$。

  - 重要性采样(Importance Sampling)

    (1) 找一个比较容易抽象的参考分布 $q(x)$ , 并令 $w(x) = \frac{p(x)}{q(x)}$ , 

    (2) 从参考分布 $q(x)$ 中抽取 $N$ 个样本 $\{x_i\}$ , 然后按照其重要性进行重新采样。

  

